# DDIM æ¨¡å‹è®­ç»ƒ - CelebA æ•°æ®é›†
from models.unet import UNet
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from models.ddim import DDIM_Model
from torchvision.datasets import ImageFolder
import torchvision.transforms as T

### 1. å‡†å¤‡ CelebA æ•°æ®é›†
transform = T.Compose([
    T.CenterCrop(178),        # CelebA çš„äººè„¸è£å‰ª
    T.Resize((64, 64)),       # 64x64 åˆ†è¾¨ç‡
    T.ToTensor(),
    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

dataset = ImageFolder(root='./data/celeba', transform=transform)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
print(f"CelebA æ•°æ®é›†å¤§å°: {len(dataset)}")

### è‡ªå®šä¹‰ DDIM æ¨¡å‹ä»¥æ”¯æŒ 64x64 å›¾åƒ
class DDIM_Model_CelebA(DDIM_Model):
    def __init__(self, dataloader, T=1000, beta_start=0.0001, beta_end=0.02, device=None):
        super().__init__(dataloader, T, beta_start, beta_end, device)
    
    @torch.no_grad()
    def sample(self, num_samples, ddim_steps=50, eta=0.0):
        """ç”Ÿæˆ 64x64 CelebA æ ·æœ¬"""
        shape = (num_samples, 3, 64, 64)  # CelebA 64x64 å›¾åƒå°ºå¯¸
        return self.p_sample_loop(shape, ddim_steps, eta)
    
    @torch.no_grad()
    def fast_sample(self, num_samples, ddim_steps=10, eta=0.0):
        """å¿«é€Ÿé‡‡æ · 64x64 å›¾åƒ"""
        return self.sample(num_samples, ddim_steps, eta)

### 2. è®­ç»ƒ DDIM æ¨¡å‹
if __name__ == "__main__":
    print("å¼€å§‹è®­ç»ƒ DDIM æ¨¡å‹ (CelebA)...")
    
    # åˆ›å»º CelebA ä¸“ç”¨ DDIM æ¨¡å‹
    ddim_model = DDIM_Model_CelebA(dataloader, T=1000, device=device)
    
    # è®­ç»ƒæ¨¡å‹ (CelebA é€šå¸¸éœ€è¦æ›´å°‘çš„ epoch å› ä¸ºæ•°æ®é›†æ›´å¤§)
    print("æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
    ddim_model.train_model(num_epochs=5, lr=1e-4, save_path="ddim_model_celeba.pth")
    
    # ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸
    torch.save(ddim_model.state_dict(), "ddim_model_celeba.pth")
    print("æ¨¡å‹å·²ä¿å­˜ä¸º: ddim_model_celeba.pth")
    
    # åˆ›å»ºæ–°æ¨¡å‹å¹¶åŠ è½½çŠ¶æ€å­—å…¸
    ddim_model_loaded = DDIM_Model_CelebA(dataloader, T=1000, device=device)
    ddim_model_loaded.load_state_dict(torch.load("ddim_model_celeba.pth"))
    
    print("å¼€å§‹ç”Ÿæˆäººè„¸æ ·æœ¬...")
    
    # ä½¿ç”¨æ ‡å‡† DDIM é‡‡æ ·ç”Ÿæˆæ ·æœ¬ (50æ­¥)
    print("ç”Ÿæˆæ ‡å‡† DDIM äººè„¸æ ·æœ¬ (50æ­¥)...")
    x_standard = ddim_model_loaded.sample(16, ddim_steps=50, eta=0.0)
    torchvision.utils.save_image(x_standard, "ddim_celeba_standard.png", normalize=True, nrow=4)
    
    # ä½¿ç”¨å¿«é€Ÿ DDIM é‡‡æ ·ç”Ÿæˆæ ·æœ¬ (10æ­¥)
    print("ç”Ÿæˆå¿«é€Ÿ DDIM äººè„¸æ ·æœ¬ (10æ­¥)...")
    x_fast = ddim_model_loaded.fast_sample(16, ddim_steps=10, eta=0.0)
    torchvision.utils.save_image(x_fast, "ddim_celeba_fast.png", normalize=True, nrow=4)
    
    # ä½¿ç”¨éšæœºæ€§é‡‡æ · (eta > 0)
    print("ç”Ÿæˆéšæœºæ€§ DDIM äººè„¸æ ·æœ¬...")
    x_stochastic = ddim_model_loaded.sample(16, ddim_steps=25, eta=0.5)
    torchvision.utils.save_image(x_stochastic, "ddim_celeba_stochastic.png", normalize=True, nrow=4)
    
    # ç”Ÿæˆæ›´å¤šæ ·æœ¬ç”¨äºå±•ç¤º
    print("ç”Ÿæˆå¤§æ‰¹é‡äººè„¸æ ·æœ¬...")
    x_batch = ddim_model_loaded.sample(64, ddim_steps=30, eta=0.2)
    torchvision.utils.save_image(x_batch, "ddim_celeba_batch.png", normalize=True, nrow=8)
    
    # è¯„ä¼°æ¨¡å‹
    print("å¼€å§‹è¯„ä¼° DDIM æ¨¡å‹...")
    fid_score = ddim_model_loaded.evaluate(num_samples=1000)
    print(f"æœ€ç»ˆ FID åˆ†æ•°: {fid_score:.2f}")
    
    print("\nğŸ‰ DDIM CelebA è®­ç»ƒå®Œæˆï¼")
    print("ç”Ÿæˆçš„å›¾åƒæ–‡ä»¶:")
    print("- ddim_celeba_standard.png (æ ‡å‡†50æ­¥é‡‡æ ·)")
    print("- ddim_celeba_fast.png (å¿«é€Ÿ10æ­¥é‡‡æ ·)")
    print("- ddim_celeba_stochastic.png (éšæœºæ€§é‡‡æ ·)")
    print("- ddim_celeba_batch.png (64å¼ äººè„¸æ ·æœ¬)")
    print("æ¨¡å‹æƒé‡: ddim_model_celeba.pth")